
"""
- Select ensemble_bundle.pkl (generated by make_ensemble_gui.py)
- Restore 5 base learners (joblib/pickle) + Transformer meta-learner (torch state_dict)
- Select Excel feature file (.xlsx/.xls; first row is header, first column ID optional)
- Optional ID column; output CSV predictions file
"""

import io
import os
import math
import pickle
import tkinter as tk
from tkinter import filedialog, messagebox, simpledialog

import joblib
import numpy as np
import pandas as pd

# ---- PyTorch for meta model ----
import torch
import torch.nn as nn

BASE_NAMES = ["ENR", "SVR", "DTR", "KNNR", "MLPR"]

# ========= Module definitions exactly matching training =========
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1024):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32)*(-math.log(10000.0)/d_model))
        pe[:, 0::2] = torch.sin(position*div_term)
        if d_model % 2 == 1:
            pe[:, 1::2] = torch.cos(position*div_term)[:, :pe[:,1::2].shape[1]]
        else:
            pe[:, 1::2] = torch.cos(position*div_term)
        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)

    def forward(self, x):
        # x: (B, F, d_model)  —— batch_first=True
        return x + self.pe[:, :x.size(1), :]

class TransformerRegressor(nn.Module):
    """
    Aligns with training script:
      value_proj: (B,F,1)->(B,F,d_model)
      feat_emb  : Embedding(n_features, d_model) column ID encoding
      Optional PositionalEncoding
      TransformerEncoder(batch_first=True)
      mean pooling -> LayerNorm -> Linear -> GELU -> Linear -> 1D output
    """
    def __init__(self, n_features, d_model, nhead, num_layers, dim_ff, dropout, use_posenc=True):
        super().__init__()
        assert d_model % nhead == 0, f"d_model={d_model} must be divisible by nhead={nhead}"

        self.n_features = n_features
        self.value_proj = nn.Linear(1, d_model)
        self.feat_emb   = nn.Embedding(n_features, d_model)
        self.use_posenc = use_posenc
        if use_posenc:
            self.pos_enc = PositionalEncoding(d_model, max_len=max(32, n_features+10))

        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,
            dropout=dropout, batch_first=True, activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
        self.head = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Linear(d_model, 1)
        )
        self.register_buffer("feat_ids", torch.arange(n_features).long())

    def forward(self, x):
        # x: (B, F) —— F=5 (ENR, SVR, DTR, KNNR, MLPR order)
        B, F = x.shape
        v = self.value_proj(x.unsqueeze(-1))          # (B,F,1)->(B,F,d)
        ids = self.feat_ids.unsqueeze(0).expand(B, F) # (B,F)
        e = self.feat_emb(ids)                        # (B,F,d)
        h = v + e
        if self.use_posenc: h = self.pos_enc(h)       # (B,F,d)
        h = self.encoder(h)                           # (B,F,d)
        h = h.mean(dim=1)                             # (B,d)
        y = self.head(h).squeeze(-1)                  # (B,)
        return y

# ========= Simple ensemble predictor =========
class EnsemblePredictor:
    def __init__(self, base_models, meta_model):
        if len(base_models) != len(BASE_NAMES):
            raise ValueError(f"{len(BASE_NAMES)} base models required, got {len(base_models)}.")
        self.base_models = base_models
        self.meta_model = meta_model  # torch.nn.Module, expects (B,5) numpy/torch input

    @staticmethod
    def _as_array(X):
        if isinstance(X, pd.DataFrame):
            return X.values
        return np.asarray(X)

    def get_base_predictions(self, X):
        Xp = self._as_array(X)
        preds = []
        for m in self.base_models:
            p = m.predict(Xp)
            preds.append(np.asarray(p).reshape(-1))
        return np.vstack(preds).T  # (B, 5)

    def predict(self, X):
        base_feats = self.get_base_predictions(X).astype(np.float32)  # (B,5)
        with torch.no_grad():
            t = torch.from_numpy(base_feats)
            y = self.meta_model(t)
            return y.detach().cpu().numpy().reshape(-1)

# ========= IO / loading tools (Excel preferred) =========
def load_excel_features(path: str) -> pd.DataFrame:
    """
    Read Excel (.xlsx/.xls), default first sheet, first row as header.
    To specify sheet, modify sheet_name parameter here.
    """
    try:
        return pd.read_excel(path, sheet_name=0, header=0)
    except ImportError as e:
        raise RuntimeError("Reading Excel requires openpyxl (.xlsx) or xlrd (.xls). Please pip install.") from e

def load_features(path: str):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".xlsx", ".xls"]:
        return load_excel_features(path)
    if ext in [".parquet", ".pq"]:
        return pd.read_parquet(path)
    if ext in [".npy", ".npz"]:
        arr = np.load(path, allow_pickle=True)
        if isinstance(arr, np.lib.npyio.NpzFile):
            key = list(arr.files)[0]
            return arr[key]
        return arr
    # CSV/TXT explicitly unsupported to avoid encoding issues
    raise ValueError("Provide Excel (.xlsx/.xls), Parquet, or NumPy feature file.")

def model_from_pickle_blob(blob: bytes):
    return joblib.load(io.BytesIO(blob))

def build_transformer_from_state_blob(blob: bytes) -> nn.Module:
    """
    Load .pt/.pth format (dict), rebuild equivalent structure and load state_dict.
    Required fields:
       - model_state
       - struct_hparams: {d_model, nhead, num_layers, dim_ff, dropout}
       - n_features
    """
    obj = torch.load(io.BytesIO(blob), map_location="cpu")
    if not (isinstance(obj, dict) and
            ("model_state" in obj) and
            ("struct_hparams" in obj) and
            ("n_features" in obj)):
        raise ValueError("Meta-learner .pt/.pth missing required fields {model_state, struct_hparams, n_features}")

    sh = obj["struct_hparams"]
    n_features = int(obj["n_features"])
    model = TransformerRegressor(
        n_features=n_features,
        d_model=int(sh["d_model"]),
        nhead=int(sh["nhead"]),
        num_layers=int(sh["num_layers"]),
        dim_ff=int(sh["dim_ff"]),
        dropout=float(sh.get("dropout", 0.0)),
        use_posenc=True
    )
    model.load_state_dict(obj["model_state"])
    model.eval()
    return model

# ========= GUI main workflow =========
def main():
    root = tk.Tk()
    root.withdraw()
    try:
        # 1) Select ensemble bundle
        messagebox.showinfo("Load Ensemble Model", "Select the bundle file (.pkl) generated by make_ensemble_gui.py.")
        bundle_path = filedialog.askopenfilename(
            title="Select ensemble_bundle.pkl",
            filetypes=[("Pickle", "*.pkl"), ("All Files", "*.*")]
        )
        if not bundle_path:
            return

        with open(bundle_path, "rb") as f:
            bundle = pickle.load(f)

        if bundle.get("format") not in {"ensemble_bundle.v3"}:
            raise ValueError("Unsupported bundle version, please regenerate using make_ensemble_gui.py.")

        # 2) Restore 5 base learners (fixed order)
        base_models = [model_from_pickle_blob(b) for b in bundle["base_blobs"]]

        # 3) Restore Transformer (state_dict format)
        if bundle.get("meta_kind") != "torch_state_v1":
            raise ValueError("Meta-learner type mismatch: expected torch_state_v1.")
        meta_model = build_transformer_from_state_blob(bundle["meta_blob"])

        ens = EnsemblePredictor(base_models, meta_model)

        # 4) Select feature file (Excel preferred)
        messagebox.showinfo("Select Features", "Select the Excel feature file (.xlsx/.xls; first row as header).")
        feat_path = filedialog.askopenfilename(
            title="Select feature file (Excel, Parquet, NumPy)",
            filetypes=[
                ("Excel", "*.xlsx *.xls"),
                ("Parquet", "*.parquet *.pq"),
                ("NumPy", "*.npy *.npz"),
                ("All Files", "*.*"),
            ]
        )
        if not feat_path:
            return

        X = load_features(feat_path)

        # 5) Optional ID column
        ids = None
        id_col = None
        if isinstance(X, pd.DataFrame):
            if messagebox.askyesno("Optional", "Specify an ID column for output? (Choose 'No' if none)"):
                cols = ", ".join(map(str, X.columns))
                id_col = simpledialog.askstring("Input ID column name", f"Available columns:\n{cols}\n\nLeave blank to skip:")
                if id_col and id_col in X.columns:
                    ids = X[id_col].copy()
                    X = X.drop(columns=[id_col])
                elif id_col:
                    messagebox.showwarning("Ignored", f"Column not found: {id_col}, ID will not be included.")
                    id_col = None

        # 6) Predict
        preds = ens.predict(X)

        # 7) Save results (CSV)
        save_path = filedialog.asksaveasfilename(
            title="Save prediction CSV",
            defaultextension=".csv",
            filetypes=[("CSV", "*.csv"), ("All Files", "*.*")]
        )
        if not save_path:
            return

        out_df = pd.DataFrame({"prediction": preds})
        if ids is not None and id_col:
            out_df.insert(0, id_col, ids.values)
        out_df.to_csv(save_path, index=False, encoding="utf-8-sig")

        messagebox.showinfo("Done", f"Predictions saved:\n{save_path}")

    except Exception as e:
        messagebox.showerror("Error", f"{e}")
        raise

if __name__ == "__main__":
    main()
